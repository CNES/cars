{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DSM to low resolution initial DEM\n",
    "\n",
    "This notebook details how to estimate and apply the transform to fit A DSM to the low resolution initial DEM. This method is currently implemented in cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters\n",
    "\n",
    "Those parameters have to be completed to use the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the cars folder\n",
    "cars_home = \"TODO\"\n",
    "# Path to the directory containing the content.json file of the prepare step output\n",
    "content_dir = \"TODO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trick to override cars verision\n",
    "import sys\n",
    "sys.path = [cars_home] + sys.path\n",
    "import os\n",
    "os.environ['OTB_APPLICATION_PATH'] = os.path.join(cars_home,'build','lib','otb','applications')+':'+os.environ['OTB_APPLICATION_PATH']\n",
    "###\n",
    "# Silent OTB info logs\n",
    "os.environ['OTB_LOGGER_LEVEL']='WARNING'\n",
    "import xarray as xr\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from cars.core import projection\n",
    "from cars.steps import triangulation\n",
    "from cars.steps import rasterization\n",
    "from cars.conf import output_prepare\n",
    "from cars.core import constants as cst\n",
    "from scipy.signal import butter, lfilter, filtfilt, lfilter_zi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres_dsm_from_matches = xr.open_dataset(os.path.join(content_dir,'lowres_dsm_from_matches.nc'))\n",
    "lowres_initial_dem = xr.open_dataset(os.path.join(content_dir,'lowres_initial_dem.nc'))\n",
    "conf = output_prepare.read_preprocessing_content_file(os.path.join(content_dir,'content.json'))\n",
    "img1 = conf['input']['img1']\n",
    "img2 = conf['input']['img2']\n",
    "dem = conf['input']['srtm_dir']\n",
    "matches = np.load(os.path.join(content_dir,'matches.npy'))\n",
    "disp_to_alt_ratio = conf['preprocessing']['output']['disp_to_alt_ratio']\n",
    "srtm_dir = conf['input']['srtm_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for direction of oscillations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the direction in which oscillations might happen (increasing acquisition time direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = projection.get_time_ground_direction(img1,dem=srtm_dir)\n",
    "vec2 = projection.get_time_ground_direction(img2, dem=srtm_dir)\n",
    "time_direction_vector = (vec1+vec2)/2\n",
    "print(\"Direction img1 {} degree wrt horizontal axis<\".format(180*math.atan2(vec1[1], vec1[0])/math.pi))\n",
    "print(\"Direction img2 {} degree wrt horizontal axis\".format(180*math.atan2(vec2[1], vec2[0])/math.pi))\n",
    "print(\"Oscillation direction: {} ({} degree wrt horizontal axis)\".format(time_direction_vector,180*math.atan2(time_direction_vector[1], time_direction_vector[0])/math.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring initial difference with low resolution DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_diff = lowres_initial_dem[cst.RASTER_HGT]-lowres_dsm_from_matches[cst.RASTER_HGT]\n",
    "(dsm_diff).plot(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting difference signal (offset and oscillations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We project differences on the  `(x,y), time_direction_vector` axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = [float(dsm_diff[cst.X][0].values), float(dsm_diff[cst.Y][0].values)]\n",
    "x_values_2d, y_values_2d = np.meshgrid(dsm_diff[cst.X], dsm_diff[cst.Y])\n",
    "curv_coords = projection.project_coordinates_on_line(x_values_2d, y_values_2d, origin, time_direction_vector)\n",
    "curv_array = xr.DataArray(dsm_diff.values.ravel(), coords={\"curv\" : curv_coords.ravel()}, dims = (\"curv\"))\n",
    "curv_array = curv_array.dropna(dim='curv')\n",
    "curv_array = curv_array.sortby('curv')\n",
    "curv_array.plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we perform denoising by aggregating median along time axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_curv = np.min(curv_array.curv)\n",
    "max_curv = np.max(curv_array.curv)\n",
    "nbins = int(math.ceil((max_curv-min_curv)/(lowres_dsm_from_matches.attrs[cst.RESOLUTION])))\n",
    "filtered_curv_array = curv_array.groupby_bins('curv',nbins).median()\n",
    "filtered_curv_array = filtered_curv_array.rename({'curv_bins': 'curv'})\n",
    "filtered_curv_array = filtered_curv_array.assign_coords({'curv' : np.array([d.mid for d in filtered_curv_array.curv.data])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute the number of point in each median slot, and discard measurements for slots with insufficient number of points (< 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_curv_npoints = curv_array.groupby_bins('curv',nbins).count()\n",
    "filtered_curv_npoints = filtered_curv_npoints.rename({'curv_bins': 'curv'})\n",
    "filtered_curv_npoints = filtered_curv_npoints.assign_coords({'curv' : np.array([d.mid for d in filtered_curv_npoints.curv.data])})\n",
    "filtered_curv_array = filtered_curv_array.where(filtered_curv_npoints > 100).dropna(dim='curv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will apply butterworth low pass filtering to extract low frequency of the difference signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, a = butter(3, 0.05)\n",
    "zi = lfilter_zi(b, a)\n",
    "z, _ = lfilter(b, a, filtered_curv_array.values, zi=zi*filtered_curv_array.values[0])\n",
    "z2, _ = lfilter(b, a, z, zi=zi*z[0])\n",
    "filtered_curv_array_lowpass = xr.DataArray(filtfilt(b, a,filtered_curv_array.values),coords=filtered_curv_array.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display both curves (median difference and low passed filtered low pass median difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.concat([filtered_curv_array,filtered_curv_array_lowpass],dim='tmp').isel(tmp=[0,1]).plot(hue='tmp',figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothed difference modelling\n",
    "We will use cubic splines to model this smoothed difference signal. Our aim is to find the smoothest cubic splines that have a RMSE < 0.3 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize s parameter\n",
    "best_s = 100*len(filtered_curv_array_lowpass.curv)\n",
    "\n",
    "# Compute first spline and RMSE\n",
    "splines = sp.interpolate.UnivariateSpline(filtered_curv_array_lowpass.curv,filtered_curv_array_lowpass.values, ext=3, k=3, s=best_s)\n",
    "estimated_correction = xr.DataArray(splines(filtered_curv_array_lowpass.curv),coords=filtered_curv_array_lowpass.coords)\n",
    "rmse = (filtered_curv_array_lowpass-estimated_correction).std(dim='curv')\n",
    "\n",
    "# Loop to find best s\n",
    "s = [best_s]\n",
    "rmses = [rmse]\n",
    "target_rmse = 0.3\n",
    "while rmse > target_rmse and best_s > 0.001:\n",
    "    best_s/=2.\n",
    "    splines = sp.interpolate.UnivariateSpline(filtered_curv_array_lowpass.curv,filtered_curv_array_lowpass.values, ext=3, k=3, s=best_s)\n",
    "    estimated_correction = xr.DataArray(splines(filtered_curv_array_lowpass.curv),coords=filtered_curv_array_lowpass.coords)\n",
    "    rmse = (filtered_curv_array_lowpass-estimated_correction).std(dim='curv')\n",
    "    s.append(best_s)\n",
    "    rmses.append(rmse)\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('value for s')\n",
    "ax.set_ylabel('rmse')\n",
    "ax.plot(rmses)\n",
    "print(\"Best smoothing factor: {} (rmse = {} meters)\".format(best_s, rmse.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can interpolate the spline to obtain the 1D correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splines = sp.interpolate.UnivariateSpline(filtered_curv_array_lowpass.curv,filtered_curv_array_lowpass.values, ext=3, k=3, s = best_s)\n",
    "estimated_correction = xr.DataArray(splines(filtered_curv_array.curv),coords=filtered_curv_array.coords)\n",
    "xr.concat([filtered_curv_array, filtered_curv_array_lowpass, estimated_correction, filtered_curv_array-estimated_correction],dim='tmp').isel(tmp=[0,1,2,3]).plot(hue='tmp',figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can interpolate the splines in 2D to get the 2D z correction field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_correction_2d = xr.DataArray(splines(curv_coords),coords=dsm_diff.coords)\n",
    "xr.concat((dsm_diff,estimated_correction_2d, lowres_initial_dem[cst.RASTER_HGT] - lowres_dsm_from_matches[cst.RASTER_HGT] - estimated_correction_2d), dim='tmp').plot(col='tmp', robust=True, figsize=(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean column is also visualized (before and after) as well as the correction (in the center)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying correction in triangulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will demonstrate how to use the correction during triangulation, using matches triangulation. first we compute the initial points cloud from matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: Data Science library notebook is not installed in interpreter Python 3.6.9 64-bit.",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter: Data Science library notebook is not installed in interpreter Python 3.6.9 64-bit.",
      "at b.startServer (/home/loic/.vscode/extensions/ms-python.python-2020.5.86806/out/client/extension.js:48:270430)",
      "at async b.createServer (/home/loic/.vscode/extensions/ms-python.python-2020.5.86806/out/client/extension.js:48:269873)",
      "at async connect (/home/loic/.vscode/extensions/ms-python.python-2020.5.86806/out/client/extension.js:48:397876)",
      "at async w.ensureConnectionAndNotebookImpl (/home/loic/.vscode/extensions/ms-python.python-2020.5.86806/out/client/extension.js:16:571752)",
      "at async w.ensureConnectionAndNotebook (/home/loic/.vscode/extensions/ms-python.python-2020.5.86806/out/client/extension.js:16:571430)",
      "at async w.clearResult (/home/loic/.vscode/extensions/ms-python.python-2020.5.86806/out/client/extension.js:16:567473)",
      "at async w.reexecuteCell (/home/loic/.vscode/extensions/ms-python.python-2020.5.86806/out/client/extension.js:16:555501)",
      "at async w.reexecuteCells (/home/loic/.vscode/extensions/ms-python.python-2020.5.86806/out/client/extension.js:16:552668)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "initial_points_cloud = triangulation.triangulate_matches(conf,matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now estimate z correction for each triangulated point using our splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_points_cloud_z_correction = splines(projection.project_coordinates_on_line(initial_points_cloud[cst.X], initial_points_cloud[cst.Y], origin, time_direction_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then convert this z correction into a disparity correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_correction = initial_points_cloud_z_correction/disp_to_alt_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this disparity correction to matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_matches = np.copy(matches)\n",
    "corrected_matches[:,2]= corrected_matches[:,2]-disp_correction[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And triangulate again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_points_cloud = triangulation.triangulate_matches(conf,corrected_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rasterize corrected cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startx = float(np.min(lowres_dsm_from_matches[cst.X]).values)-0.5*lowres_dsm_from_matches.resolution\n",
    "starty = float(np.max(lowres_dsm_from_matches[cst.Y]).values)+0.5*lowres_dsm_from_matches.resolution\n",
    "sizex = lowres_dsm_from_matches[cst.X].shape[0]\n",
    "sizey = lowres_dsm_from_matches[cst.Y].shape[0]\n",
    "corrected_lowres_dsm_from_matches = rasterization.simple_rasterization_dataset([corrected_points_cloud],lowres_dsm_from_matches.resolution,xstart=startx, ystart=starty, xsize=sizex, ysize=sizey, epsg=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we display diff with low resolution intial dem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.concat((lowres_initial_dem[cst.RASTER_HGT]-lowres_dsm_from_matches[cst.RASTER_HGT],\n",
    "           lowres_initial_dem[cst.RASTER_HGT]-corrected_lowres_dsm_from_matches[cst.RASTER_HGT]), dim='tmp').plot(col='tmp', robust=True,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cars-python-3.7-pandora-v1.b",
   "language": "python",
   "name": "cars-python-3.7-pandora-v1.b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}